# Pipeline de DonnÃ©es Cryptomonnaies - Airflow + Spark + PostgreSQL

## Table des matiÃ¨res

- [PrÃ©sentation du projet](#prÃ©sentation-du-projet)
- [Architecture du systÃ¨me](#architecture-du-systÃ¨me)
- [Technologies utilisÃ©es](#technologies-utilisÃ©es)
- [PrÃ©requis](#prÃ©requis)
- [Installation de A Ã  Z](#installation-de-a-Ã -z)
- [Structure du projet](#structure-du-projet)
- [Configuration](#configuration)
- [DÃ©marrage du systÃ¨me](#dÃ©marrage-du-systÃ¨me)
- [Les DAGs Airflow](#les-dags-airflow)
- [Utilisation](#utilisation)
- [Monitoring et Logs](#monitoring-et-logs)
- [Troubleshooting](#troubleshooting)
- [Auteurs](#auteurs)

---

## PrÃ©sentation du projet

Ce projet implÃ©mente un **pipeline de donnÃ©es automatisÃ©** pour collecter, traiter et analyser en temps rÃ©el les donnÃ©es des cryptomonnaies depuis l'API CoinCap. Le systÃ¨me orchestre l'extraction, la transformation et le chargement (ETL) des donnÃ©es avec Apache Airflow, traite les donnÃ©es avec Apache Spark (Scala), et stocke le tout dans une base PostgreSQL.

### Objectifs

- Collecter automatiquement les donnÃ©es des cryptomonnaies toutes les 30 minutes
- Traiter et transformer les donnÃ©es avec Apache Spark
- Stocker les donnÃ©es dans PostgreSQL pour analyse
- Monitorer les performances et dÃ©tecter les anomalies
- Envoyer des alertes automatiques par email
- GÃ©nÃ©rer des rapports quotidiens

---

## Architecture du systÃ¨me

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     APACHE AIRFLOW                              â”‚
â”‚                 (Orchestration & Scheduling)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â”œâ”€â”€â”€â”€â”€â–º DAG 1: crypto_data_pipeline (30 min)
              â”‚       â”œâ”€ Check API CoinCap
              â”‚       â”œâ”€ Check Database Connection
              â”‚       â”œâ”€ Cleanup Old Data (7 days)
              â”‚       â”œâ”€ Spark Processing (Scala)
              â”‚       â”œâ”€ Verify Results
              â”‚       â”œâ”€ Data Quality Check
              â”‚       â”œâ”€ Generate Daily Report
              â”‚       â””â”€ Send Email Notification
              â”‚
              â””â”€â”€â”€â”€â”€â–º DAG 2: crypto_monitoring (hourly)
                      â”œâ”€ Check Top 10 Cryptos
                      â”œâ”€ Detect Anomalies
                      â””â”€ Send Alerts

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  CoinCap API   â”‚
         â”‚ (Data Source)  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Apache Spark  â”‚
         â”‚   (Scala JAR)  â”‚
         â”‚   Processing   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   PostgreSQL   â”‚
         â”‚   (Storage)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Technologies utilisÃ©es

| Technologie | Version | Utilisation |
|-------------|---------|-------------|
| **Apache Airflow** | 2.10.2 | Orchestration des pipelines |
| **Apache Spark** | 3.x | Traitement distribuÃ© des donnÃ©es (Scala) |
| **PostgreSQL** | 14+ | Base de donnÃ©es relationnelle |
| **Python** | 3.12 | Langage principal pour Airflow |
| **Scala** | 2.13 | Job Spark |
| **Pandas** | 2.3+ | Analyse de donnÃ©es Python |
| **psycopg2** | 2.9+ | Connecteur PostgreSQL |
| **requests** | 2.32+ | RequÃªtes HTTP API |

### API externe
- **CoinCap API v2** : Source de donnÃ©es des cryptomonnaies (https://api.coincap.io/v2)

---

## PrÃ©requis

### Logiciels requis

1. **Python 3.12+**
   ```bash
   python3 --version
   ```

2. **PostgreSQL 14+**
   ```bash
   psql --version
   ```

3. **Java 11+** (pour Spark)
   ```bash
   java -version
   ```

4. **Apache Spark** (inclus dans le projet via le JAR)

5. **Git** (pour cloner le projet)

---

## Installation de A Ã  Z

### Ã‰tape 1 : Cloner le projet

```bash
cd ~/Documents
git clone <votre-repo>
cd spark_airflow
```

### Ã‰tape 2 : CrÃ©er et activer l'environnement virtuel Python

```bash
# CrÃ©er l'environnement virtuel
python3 -m venv airflow_env

# Activer l'environnement
source airflow_env/bin/activate
```

### Ã‰tape 3 : Installer Apache Airflow et dÃ©pendances

**Option 1 : Installation rapide avec requirements.txt**
```bash
# DÃ©finir la variable d'environnement pour Airflow
export AIRFLOW_HOME=$(pwd)

# Installer toutes les dÃ©pendances
pip install -r requirements.txt
```

**Option 2 : Installation manuelle**
```bash
# DÃ©finir la variable d'environnement pour Airflow
export AIRFLOW_HOME=$(pwd)

# Installer Airflow
pip install "apache-airflow==2.10.2" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-2.10.2/constraints-3.12.txt"

# Installer les providers nÃ©cessaires
pip install apache-airflow-providers-apache-spark==4.1.0
pip install apache-airflow-providers-postgres==5.5.1

# Installer les dÃ©pendances Python supplÃ©mentaires
pip install pandas psycopg2-binary requests streamlit plotly
```

### Ã‰tape 4 : Initialiser la base de donnÃ©es Airflow

```bash
# Initialiser la base de donnÃ©es Airflow (SQLite par dÃ©faut)
airflow db init
```

### Ã‰tape 5 : CrÃ©er un utilisateur admin Airflow

```bash
airflow users create \
    --username admin \
    --firstname Komi \
    --lastname Missiamenou \
    --role Admin \
    --email komimissiamenou97@gmail.com \
    --password admin
```

### Ã‰tape 6 : Configurer PostgreSQL

```bash
# Se connecter Ã  PostgreSQL
psql -U postgres

# Dans le shell PostgreSQL, crÃ©er la base de donnÃ©es
CREATE DATABASE crypto_db;

# Se connecter Ã  la base
\c crypto_db

# ExÃ©cuter le script de crÃ©ation des tables
\i scripts/bd.sql

# Quitter
\q
```

### Ã‰tape 7 : Configurer les variables Airflow

Dans l'interface web Airflow (aprÃ¨s dÃ©marrage), aller dans **Admin â†’ Variables** et ajouter :

| Key | Value | Description |
|-----|-------|-------------|
| `coincap_url` | `https://api.coincap.io/v2/assets` | URL de l'API CoinCap |
| `postgres_url` | `jdbc:postgresql://localhost:5432/crypto_db` | URL JDBC PostgreSQL |
| `pg_user` | `postgres` | Utilisateur PostgreSQL |
| `pg_password` | `postgres` | Mot de passe PostgreSQL |

### Ã‰tape 8 : Configurer les connexions Airflow

Dans **Admin â†’ Connections**, ajouter :

#### Connection PostgreSQL
- **Conn Id** : `postgres_default`
- **Conn Type** : `Postgres`
- **Host** : `localhost`
- **Database** : `crypto_db`
- **Login** : `postgres`
- **Password** : `postgres`
- **Port** : `5432`

#### Connection Spark
- **Conn Id** : `spark_default`
- **Conn Type** : `Spark`
- **Host** : `local[*]`
- **Extra** : `{"deploy-mode": "client"}`

---

## Structure du projet

```
spark_airflow/
â”‚
â”œâ”€â”€ airflow_env/                    # Environnement virtuel Python
â”‚   â”œâ”€â”€ bin/
â”‚   â”œâ”€â”€ lib/
â”‚   â””â”€â”€ pyvenv.cfg
â”‚
â”œâ”€â”€ dags/                           # DAGs Airflow
â”‚   â”œâ”€â”€ dag_import_data.py         # Pipeline principal (30 min)
â”‚   â””â”€â”€ dag_monitoring_data.py     # Monitoring (hourly)
â”‚
â”œâ”€â”€ jars/                           # JARs Spark
â”‚   â””â”€â”€ crypto-spark-processor.jar # Job Spark Scala compilÃ©
â”‚
â”œâ”€â”€ config/                         # Fichiers de configuration
â”‚   â””â”€â”€ config.properties          # Configuration CoinCap + PostgreSQL
â”‚
â”œâ”€â”€ scripts/                        # Scripts SQL
â”‚   â””â”€â”€ bd.sql                     # SchÃ©ma de la base de donnÃ©es
â”‚
â”œâ”€â”€ logs/                           # Logs Airflow (gÃ©nÃ©rÃ© automatiquement)
â”‚   â””â”€â”€ scheduler/
â”‚
â”œâ”€â”€ airflow.cfg                     # Configuration Airflow
â”œâ”€â”€ airflow.db                      # Base SQLite Airflow
â””â”€â”€ README.md                       # Ce fichier
```

---

## Configuration

### 1. Configuration CoinCap API

Le fichier `config/config.properties` contient :

```properties
# Configuration CoinCap API
COINCAP_API_KEY=e16134f9b6ec6787a1e70bee66c5ee26dbe2f4c4b8171ffeda56bb05c1d5cfba
COINCAP_BASE_URL=https://api.coincap.io/v2

# Configuration PostgreSQL
POSTGRES_URL=jdbc:postgresql://localhost:5432/crypto_db
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
```

### 2. Configuration Email (optionnel)

Pour recevoir les notifications par email, modifier `airflow.cfg` :

```ini
[smtp]
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
smtp_user = votre-email@gmail.com
smtp_password = votre-mot-de-passe-app
smtp_port = 587
smtp_mail_from = votre-email@gmail.com
```

**Note** : Pour Gmail, crÃ©er un "Mot de passe d'application" dans les paramÃ¨tres de sÃ©curitÃ©.

---

## ğŸ¬ DÃ©marrage du systÃ¨me

### Option 1 : DÃ©marrage manuel (2 terminaux)

#### Terminal 1 - Webserver Airflow
```bash
cd /Users/komidoveneymissiamenou/Documents/Master\ 1\ DIT/DataEnge/spark_airflow
source airflow_env/bin/activate
export AIRFLOW_HOME=$(pwd)
airflow webserver --port 8080
```

#### Terminal 2 - Scheduler Airflow
```bash
cd /Users/komidoveneymissiamenou/Documents/Master\ 1\ DIT/DataEnge/spark_airflow
source airflow_env/bin/activate
export AIRFLOW_HOME=$(pwd)
airflow scheduler
```

### Option 2 : DÃ©marrage en arriÃ¨re-plan

```bash
# DÃ©marrer le webserver en arriÃ¨re-plan
nohup airflow webserver --port 8080 > logs/webserver.log 2>&1 &

# DÃ©marrer le scheduler en arriÃ¨re-plan
nohup airflow scheduler > logs/scheduler.log 2>&1 &
```

### AccÃ©der Ã  l'interface Airflow

Ouvrir le navigateur : **http://localhost:8080**

**Identifiants** :
- Username : `admin`
- Password : `admin`

---

## Les DAGs Airflow

### DAG 1 : `crypto_data_pipeline`

**FrÃ©quence** : Toutes les 30 minutes

**Workflow** :
```
start
  â”œâ”€â–º check_coincap_api          (VÃ©rifier l'API CoinCap)
  â””â”€â–º check_database_connection  (VÃ©rifier PostgreSQL)
       â†“
  cleanup_old_data               (Nettoyer donnÃ©es > 7 jours)
       â†“
  spark_crypto_processing        (Job Spark Scala - Traitement)
       â†“
  verify_spark_results           (VÃ©rifier rÃ©sultats Spark)
       â†“
  verify_data_quality            (ContrÃ´le qualitÃ© Top 10)
       â†“
  generate_daily_report          (GÃ©nÃ©rer rapport quotidien)
       â†“
  send_success_email             (Email de notification)
       â†“
  end
```

**FonctionnalitÃ©s** :
- Collecte des donnÃ©es des cryptos depuis CoinCap
- Traitement avec Spark (transformation, nettoyage)
- Stockage dans PostgreSQL
- VÃ©rification de la qualitÃ© des donnÃ©es
- GÃ©nÃ©ration de rapports
- Notifications email

### DAG 2 : `crypto_monitoring`

**FrÃ©quence** : Toutes les heures

**Workflow** :
```
start
  â†“
check_top_10_cryptos            (Analyser Top 10)
  â†“
detect_anomalies                (DÃ©tection d'anomalies)
  â†“
send_alert_if_needed            (Alerte si nÃ©cessaire)
  â†“
end
```

**FonctionnalitÃ©s** :
- Surveillance des variations importantes (> 10%)
- DÃ©tection d'anomalies
- Alertes automatiques par email

---

## Dashboard Streamlit

### AccÃ©der au Dashboard

Le projet inclut un **dashboard interactif Streamlit** pour visualiser vos donnÃ©es crypto en temps rÃ©el !

#### DÃ©marrage rapide

**Option 1 : Script automatique**
```bash
./run_dashboard.sh
```

**Option 2 : Manuel**
```bash
cd /Users/komidoveneymissiamenou/Documents/Master\ 1\ DIT/DataEnge/spark_airflow
source airflow_env/bin/activate
streamlit run dashboard.py
```

#### AccÃ¨s au Dashboard

Une fois lancÃ©, ouvrez votre navigateur : **http://localhost:8501**

### FonctionnalitÃ©s du Dashboard

Le dashboard propose plusieurs visualisations :

#### Top 3 Cryptomonnaies
- Affichage en temps rÃ©el des 3 meilleures cryptos
- Prix actuel et variation 24h avec couleurs (vert/rouge)

#### Statistiques Globales
- **Market Cap Total** : Capitalisation totale du marchÃ©
- **Variation Moyenne 24h** : Performance moyenne de toutes les cryptos
- **Cryptos en hausse/baisse** : Compteurs en temps rÃ©el

#### Graphiques Interactifs
1. **Graphique en barres** : Variation 24h du Top 10
2. **Graphique circulaire** : RÃ©partition du Market Cap

#### Tableau des Cryptomonnaies
- Liste complÃ¨te avec filtres :
  - Seulement les hausses
  - Seulement les baisses
  - Variation minimale personnalisÃ©e
- Tri et recherche
- Colonnes colorÃ©es selon les variations

#### Analyse DÃ©taillÃ©e
- SÃ©lection d'une crypto spÃ©cifique
- **Graphique d'Ã©volution** : Prix historique sur X jours (configurable)
- **Statistiques** :
  - Prix minimum/maximum sur la pÃ©riode
  - Variation totale sur X jours

#### Statistiques du Pipeline
- DerniÃ¨res opÃ©rations Airflow
- Graphique des opÃ©rations par type
- Logs en temps rÃ©el

### Personnalisation

Dans la **barre latÃ©rale** du dashboard :
- Bouton de rafraÃ®chissement manuel
- Curseur pour sÃ©lectionner la pÃ©riode d'analyse (1-30 jours)
- Informations sur le pipeline

### Captures d'Ã©cran

Le dashboard affiche :
- MÃ©triques en temps rÃ©el
- Graphiques interactifs (zoom, pan, export)
- Couleurs adaptatives (vert pour hausse, rouge pour baisse)
- Design responsive et moderne
- Mise Ã  jour automatique des donnÃ©es (cache 60s)

---

## Utilisation

### 1. Activer les DAGs

Dans l'interface Airflow (http://localhost:8080) :
1. Aller sur la page d'accueil
2. Rechercher `crypto_data_pipeline` et `crypto_monitoring`
3. Cliquer sur le **toggle** Ã  gauche pour les activer (devient bleu)

### 2. ExÃ©cuter manuellement un DAG

1. Cliquer sur le nom du DAG
2. Cliquer sur le bouton **Play** (en haut Ã  droite)
3. Confirmer l'exÃ©cution

### 3. Visualiser l'exÃ©cution

- **Grid View** : Vue d'ensemble des exÃ©cutions
- **Graph View** : Vue graphique du workflow
- **Task Duration** : DurÃ©e d'exÃ©cution des tÃ¢ches
- **Gantt** : Diagramme de Gantt

### 4. Consulter les logs

Pour chaque tÃ¢che :
1. Cliquer sur la tÃ¢che dans le Graph View
2. Cliquer sur **Log**
3. Les logs s'affichent en temps rÃ©el

---

## Monitoring et Logs

### Logs Airflow

Les logs sont stockÃ©s dans le dossier `logs/` :

```bash
# Voir les logs du scheduler
tail -f logs/scheduler/latest/*.log

# Voir les logs d'une tÃ¢che spÃ©cifique
tail -f logs/dag_id=crypto_data_pipeline/run_id=*/task_id=*/attempt=*.log
```

### Monitoring PostgreSQL

```bash
# Se connecter Ã  la base
psql -U postgres -d crypto_db

# Voir les derniÃ¨res donnÃ©es importÃ©es
SELECT * FROM crypto_temps_reel 
WHERE date_import = CURRENT_DATE 
ORDER BY rang LIMIT 10;

# Statistiques du pipeline
SELECT * FROM pipeline_logs 
ORDER BY timestamp DESC LIMIT 20;
```

### Consulter les rapports

Les rapports quotidiens sont gÃ©nÃ©rÃ©s dans les logs de la tÃ¢che `generate_daily_report` :

```
RAPPORT CRYPTO QUOTIDIEN - 2025-10-25
============================================

TOP 10 CRYPTOMONNAIES:
rang  symbole  nom         prix_usd    variation_24h_pourcent
1     BTC      Bitcoin     67234.50    +2.45
2     ETH      Ethereum    3567.89     +3.12
...

Market Cap Total: $2.45 Md
Performance Moyenne: +2.67%
DerniÃ¨re mise Ã  jour: 21:59:50
```

---

## Troubleshooting

### ProblÃ¨me : DAG cassÃ© (Broken DAG)

**Erreur** : `ModuleNotFoundError: No module named 'pandas'`

**Solution** :
```bash
source airflow_env/bin/activate
pip install pandas psycopg2-binary requests
# RedÃ©marrer le scheduler
pkill -f "airflow scheduler"
airflow scheduler
```

---

### ProblÃ¨me : Providers manquants

**Erreur** : `No module named 'airflow.providers.apache.spark'`

**Solution** :
```bash
pip install apache-airflow-providers-apache-spark
pip install apache-airflow-providers-postgres
# RedÃ©marrer Airflow
```

---

### ProblÃ¨me : Connexion PostgreSQL Ã©choue

**Erreur** : `could not connect to server`

**Solution** :
```bash
# VÃ©rifier que PostgreSQL tourne
brew services list  # macOS
systemctl status postgresql  # Linux

# DÃ©marrer PostgreSQL si nÃ©cessaire
brew services start postgresql  # macOS
systemctl start postgresql  # Linux

# Tester la connexion
psql -U postgres -d crypto_db
```

---

### ProblÃ¨me : API CoinCap non accessible

**Erreur** : `API CoinCap erreur: 429 Too Many Requests`

**Solution** : Vous avez atteint la limite de requÃªtes. Attendre quelques minutes ou augmenter l'intervalle du DAG.

---

### ProblÃ¨me : Spark job Ã©choue

**Erreur** : `FileNotFoundException: crypto-spark-processor.jar`

**Solution** :
1. VÃ©rifier que le JAR existe dans `jars/crypto-spark-processor.jar`
2. VÃ©rifier le chemin dans le DAG (ligne 177 de `dag_import_data.py`)
3. Le chemin doit Ãªtre absolu ou relatif Ã  AIRFLOW_HOME

---

### ProblÃ¨me : Scheduler ne dÃ©marre pas

**Erreur** : `Address already in use`

**Solution** :
```bash
# Trouver le processus qui utilise le port
lsof -i :8793

# Tuer le processus
kill -9 <PID>

# RedÃ©marrer le scheduler
airflow scheduler
```

---

## Commandes utiles

### Airflow

```bash
# Lister les DAGs
airflow dags list

# Tester un DAG
airflow dags test crypto_data_pipeline 2025-10-25

# Voir les erreurs d'import
airflow dags list-import-errors

# Lister les tÃ¢ches d'un DAG
airflow tasks list crypto_data_pipeline

# Tester une tÃ¢che spÃ©cifique
airflow tasks test crypto_data_pipeline check_coincap_api 2025-10-25

# DÃ©clencher un DAG manuellement
airflow dags trigger crypto_data_pipeline

# Mettre en pause un DAG
airflow dags pause crypto_data_pipeline

# RÃ©activer un DAG
airflow dags unpause crypto_data_pipeline
```

### PostgreSQL

```bash
# Se connecter Ã  la base
psql -U postgres -d crypto_db

# RequÃªtes SQL utiles
-- Nombre de cryptos par jour
SELECT date_import, COUNT(*) 
FROM crypto_temps_reel 
GROUP BY date_import 
ORDER BY date_import DESC;

-- Top 10 cryptos actuelles
SELECT rang, symbole, nom, prix_usd, variation_24h_pourcent
FROM crypto_temps_reel
WHERE date_import = CURRENT_DATE
ORDER BY rang
LIMIT 10;

-- Historique d'une crypto
SELECT date_import, heure_import, prix_usd, variation_24h_pourcent
FROM crypto_temps_reel
WHERE symbole = 'BTC'
ORDER BY date_import DESC, heure_import DESC
LIMIT 20;
```

---

## Contact et Support

### Ã‰quipe du projet

**Groupe 3 - Master 1 DIT**

- **Komi Missiamenou** - komimissiamenou97@gmail.com
- **Sabet Shibangua Lidor** - sabuetshibangualidor@gmail.com

### Support

En cas de problÃ¨me :
1. Consulter la section [Troubleshooting](#-troubleshooting)
2. VÃ©rifier les logs Airflow
3. Contacter l'Ã©quipe

---

## Contexte acadÃ©mique

**Projet rÃ©alisÃ© dans le cadre de** :
- Formation : Master 1 DIT (Data & Intelligence Technologique)
- Module : Data Engineering
- AnnÃ©e : 2025

**Objectifs pÃ©dagogiques** :
- Orchestration de pipelines de donnÃ©es avec Apache Airflow
- Traitement distribuÃ© avec Apache Spark
- IntÃ©gration de sources de donnÃ©es externes (API REST)
- Stockage et requÃªtage avec PostgreSQL
- Monitoring et alerting automatisÃ©s

---

## Licence

Ce projet est rÃ©alisÃ© Ã  des fins Ã©ducatives dans le cadre du Master 1 DIT.

---

## Remerciements

- **CoinCap API** pour la fourniture gratuite des donnÃ©es cryptomonnaies
- **Apache Software Foundation** pour Airflow et Spark
- **PostgreSQL Global Development Group**
- Nos enseignants du Master DIT

---

## Notes importantes

1. **SÃ©curitÃ©** : Ne jamais exposer les mots de passe et API keys en production. Utiliser des secrets managers (AWS Secrets Manager, HashiCorp Vault, etc.)

2. **Performance** : En production, remplacer SQLite par PostgreSQL pour la base Airflow elle-mÃªme

3. **ScalabilitÃ©** : Pour de plus gros volumes, utiliser CeleryExecutor ou KubernetesExecutor au lieu de SequentialExecutor

4. **Backup** : Mettre en place des sauvegardes rÃ©guliÃ¨res de la base PostgreSQL

5. **Monitoring** : En production, intÃ©grer des outils comme Prometheus/Grafana pour le monitoring avancÃ©

---

**DerniÃ¨re mise Ã  jour** : 25 octobre 2025

**Version** : 1.0.0

---

*Made by Groupe 3 - Master 1 DIT*

